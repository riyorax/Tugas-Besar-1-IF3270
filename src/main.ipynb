{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF3270 Pembelajaran Mesin | Tugas Besar - Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Members:\n",
    "- Maximilian Sulistiyo (13522061)\n",
    "- Marvel Pangondian (13522075)\n",
    "- Abdullah Mubarak (13522101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we implement a custom built Feedforward Neural Network with no high-level libraries. The goal in this project is to be able to create a custom FFNN that is able to specify the type of activation function on each layer, the type of loss function, and how many neurons in each layer. We will also compare our algorithm with other built in algorithm (the sklearn MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ann import NeuralNetwork, one_hot, get_accuracy\n",
    "from dense_layer import DenseLayer\n",
    "from activations import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from activations import tanh, sigmoid, relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy dataset and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original = X.copy()\n",
    "X_original = X_original/255.0\n",
    "y_original = y.copy()\n",
    "y_original = y_original.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_original, y_original, train_size=60000, test_size=10000, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_oh = one_hot(y_train)\n",
    "y_test_oh = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - 1.87s - loss: 0.0989\n",
      "Epoch 2/100 - 1.79s - loss: 0.0797\n",
      "Epoch 3/100 - 1.73s - loss: 0.0615\n",
      "Epoch 4/100 - 1.74s - loss: 0.0482\n",
      "Epoch 5/100 - 1.76s - loss: 0.0393\n",
      "Epoch 6/100 - 1.81s - loss: 0.0328\n",
      "Epoch 7/100 - 1.82s - loss: 0.0284\n",
      "Epoch 8/100 - 1.87s - loss: 0.0255\n",
      "Epoch 9/100 - 1.86s - loss: 0.0233\n",
      "Epoch 10/100 - 1.82s - loss: 0.0217\n",
      "Epoch 11/100 - 1.91s - loss: 0.0205\n",
      "Epoch 12/100 - 2.10s - loss: 0.0195\n",
      "Epoch 13/100 - 1.82s - loss: 0.0187\n",
      "Epoch 14/100 - 1.97s - loss: 0.0180\n",
      "Epoch 15/100 - 2.12s - loss: 0.0174\n",
      "Epoch 16/100 - 2.15s - loss: 0.0168\n",
      "Epoch 17/100 - 1.93s - loss: 0.0163\n",
      "Epoch 18/100 - 1.88s - loss: 0.0159\n",
      "Epoch 19/100 - 1.94s - loss: 0.0155\n",
      "Epoch 20/100 - 1.90s - loss: 0.0152\n",
      "Epoch 21/100 - 1.94s - loss: 0.0148\n",
      "Epoch 22/100 - 2.04s - loss: 0.0145\n",
      "Epoch 23/100 - 1.97s - loss: 0.0142\n",
      "Epoch 24/100 - 1.86s - loss: 0.0140\n",
      "Epoch 25/100 - 1.74s - loss: 0.0137\n",
      "Epoch 26/100 - 1.66s - loss: 0.0135\n",
      "Epoch 27/100 - 1.79s - loss: 0.0132\n",
      "Epoch 28/100 - 1.76s - loss: 0.0130\n",
      "Epoch 29/100 - 1.71s - loss: 0.0128\n",
      "Epoch 30/100 - 1.80s - loss: 0.0126\n",
      "Epoch 31/100 - 1.94s - loss: 0.0124\n",
      "Epoch 32/100 - 1.79s - loss: 0.0122\n",
      "Epoch 33/100 - 2.00s - loss: 0.0121\n",
      "Epoch 34/100 - 1.80s - loss: 0.0119\n",
      "Epoch 35/100 - 1.84s - loss: 0.0117\n",
      "Epoch 36/100 - 1.85s - loss: 0.0116\n",
      "Epoch 37/100 - 1.79s - loss: 0.0114\n",
      "Epoch 38/100 - 1.90s - loss: 0.0113\n",
      "Epoch 39/100 - 1.94s - loss: 0.0112\n",
      "Epoch 40/100 - 1.90s - loss: 0.0110\n",
      "Epoch 41/100 - 2.06s - loss: 0.0109\n",
      "Epoch 42/100 - 2.03s - loss: 0.0108\n",
      "Epoch 43/100 - 2.23s - loss: 0.0106\n",
      "Epoch 44/100 - 1.99s - loss: 0.0105\n",
      "Epoch 45/100 - 1.93s - loss: 0.0104\n",
      "Epoch 46/100 - 1.98s - loss: 0.0103\n",
      "Epoch 47/100 - 2.13s - loss: 0.0102\n",
      "Epoch 48/100 - 2.14s - loss: 0.0100\n",
      "Epoch 49/100 - 2.14s - loss: 0.0099\n",
      "Epoch 50/100 - 2.14s - loss: 0.0098\n",
      "Epoch 51/100 - 1.73s - loss: 0.0097\n",
      "Epoch 52/100 - 1.78s - loss: 0.0096\n",
      "Epoch 53/100 - 1.73s - loss: 0.0095\n",
      "Epoch 54/100 - 1.76s - loss: 0.0094\n",
      "Epoch 55/100 - 1.79s - loss: 0.0093\n",
      "Epoch 56/100 - 1.72s - loss: 0.0093\n",
      "Epoch 57/100 - 1.88s - loss: 0.0092\n",
      "Epoch 58/100 - 1.96s - loss: 0.0091\n",
      "Epoch 59/100 - 2.02s - loss: 0.0090\n",
      "Epoch 60/100 - 2.04s - loss: 0.0089\n",
      "Epoch 61/100 - 2.07s - loss: 0.0088\n",
      "Epoch 62/100 - 2.07s - loss: 0.0087\n",
      "Epoch 63/100 - 2.12s - loss: 0.0087\n",
      "Epoch 64/100 - 2.00s - loss: 0.0086\n",
      "Epoch 65/100 - 1.93s - loss: 0.0085\n",
      "Epoch 66/100 - 2.05s - loss: 0.0084\n",
      "Epoch 67/100 - 2.02s - loss: 0.0083\n",
      "Epoch 68/100 - 2.06s - loss: 0.0083\n",
      "Epoch 69/100 - 2.06s - loss: 0.0082\n",
      "Epoch 70/100 - 2.13s - loss: 0.0081\n",
      "Epoch 71/100 - 2.06s - loss: 0.0081\n",
      "Epoch 72/100 - 2.06s - loss: 0.0080\n",
      "Epoch 73/100 - 2.45s - loss: 0.0079\n",
      "Epoch 74/100 - 2.33s - loss: 0.0079\n",
      "Epoch 75/100 - 2.24s - loss: 0.0078\n",
      "Epoch 76/100 - 2.20s - loss: 0.0077\n",
      "Epoch 77/100 - 2.00s - loss: 0.0077\n",
      "Epoch 78/100 - 1.88s - loss: 0.0076\n",
      "Epoch 79/100 - 1.91s - loss: 0.0075\n",
      "Epoch 80/100 - 1.79s - loss: 0.0075\n",
      "Epoch 81/100 - 1.86s - loss: 0.0074\n",
      "Epoch 82/100 - 1.77s - loss: 0.0074\n",
      "Epoch 83/100 - 1.96s - loss: 0.0073\n",
      "Epoch 84/100 - 1.80s - loss: 0.0073\n",
      "Epoch 85/100 - 1.86s - loss: 0.0072\n",
      "Epoch 86/100 - 1.85s - loss: 0.0071\n",
      "Epoch 87/100 - 1.84s - loss: 0.0071\n",
      "Epoch 88/100 - 1.87s - loss: 0.0070\n",
      "Epoch 89/100 - 1.92s - loss: 0.0070\n",
      "Epoch 90/100 - 2.14s - loss: 0.0069\n",
      "Epoch 91/100 - 1.95s - loss: 0.0069\n",
      "Epoch 92/100 - 1.93s - loss: 0.0068\n",
      "Epoch 93/100 - 1.98s - loss: 0.0068\n",
      "Epoch 94/100 - 2.15s - loss: 0.0067\n",
      "Epoch 95/100 - 2.05s - loss: 0.0067\n",
      "Epoch 96/100 - 2.04s - loss: 0.0066\n",
      "Epoch 97/100 - 2.05s - loss: 0.0066\n",
      "Epoch 98/100 - 2.12s - loss: 0.0065\n",
      "Epoch 99/100 - 2.24s - loss: 0.0065\n",
      "Epoch 100/100 - 2.46s - loss: 0.0065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.09887546385797048,\n",
       "  0.07965197314605793,\n",
       "  0.0615380863021446,\n",
       "  0.04818587192802164,\n",
       "  0.03929758146076872,\n",
       "  0.03282052751124533,\n",
       "  0.028445561648181978,\n",
       "  0.025452193065893945,\n",
       "  0.023327418723962417,\n",
       "  0.021735940598507753,\n",
       "  0.02049672342264637,\n",
       "  0.019509111231029715,\n",
       "  0.018678546067509096,\n",
       "  0.017971643722024516,\n",
       "  0.01736136759714431,\n",
       "  0.016826387746934738,\n",
       "  0.016344665049137404,\n",
       "  0.015913710620197034,\n",
       "  0.015526006377868887,\n",
       "  0.015166287754794089,\n",
       "  0.014829212624838406,\n",
       "  0.014517471090078487,\n",
       "  0.014232228385742892,\n",
       "  0.013962852874065573,\n",
       "  0.01370337689221501,\n",
       "  0.013468673084950013,\n",
       "  0.013237415466681966,\n",
       "  0.013020250379980659,\n",
       "  0.01282030265402161,\n",
       "  0.012620597833985463,\n",
       "  0.012428999676826877,\n",
       "  0.012247858386089439,\n",
       "  0.012077063356956897,\n",
       "  0.011912167725123728,\n",
       "  0.011748543002166293,\n",
       "  0.011605521211064694,\n",
       "  0.01144576889549084,\n",
       "  0.011295389455574333,\n",
       "  0.011158907111500624,\n",
       "  0.011017806494414376,\n",
       "  0.01089351182873042,\n",
       "  0.0107556341812861,\n",
       "  0.010637333159911251,\n",
       "  0.010516518468590441,\n",
       "  0.010393807989598613,\n",
       "  0.01027765270292978,\n",
       "  0.010163394344634617,\n",
       "  0.010049351624443825,\n",
       "  0.009942370681192686,\n",
       "  0.009835615477151408,\n",
       "  0.009733705075695012,\n",
       "  0.00964184056301682,\n",
       "  0.009534022068187509,\n",
       "  0.009438225211109675,\n",
       "  0.009347887076949899,\n",
       "  0.00925724542633682,\n",
       "  0.009162500172476627,\n",
       "  0.00907179688996535,\n",
       "  0.008985235660866937,\n",
       "  0.008896104646606631,\n",
       "  0.008820081995546635,\n",
       "  0.008739825329258114,\n",
       "  0.008658655239370676,\n",
       "  0.008576736279252376,\n",
       "  0.008499269638183761,\n",
       "  0.008426391948948751,\n",
       "  0.008345935105764469,\n",
       "  0.008274712748923973,\n",
       "  0.008206774734174303,\n",
       "  0.008130371342336682,\n",
       "  0.008065332993167574,\n",
       "  0.007993329412987485,\n",
       "  0.007924971385279131,\n",
       "  0.007867717117419182,\n",
       "  0.007794994697599339,\n",
       "  0.007732760114275518,\n",
       "  0.007673938457446226,\n",
       "  0.007609870504185806,\n",
       "  0.007543351355257758,\n",
       "  0.007488606951563766,\n",
       "  0.007425834207090369,\n",
       "  0.007373279719123838,\n",
       "  0.007315847004721829,\n",
       "  0.007254093528353343,\n",
       "  0.007200446677071301,\n",
       "  0.007145346883782296,\n",
       "  0.007091348941976735,\n",
       "  0.0070370822909214086,\n",
       "  0.006983588461783947,\n",
       "  0.006934118930161342,\n",
       "  0.006880924844864883,\n",
       "  0.006833595745112947,\n",
       "  0.006781995838532821,\n",
       "  0.00673286359758394,\n",
       "  0.006684949824449962,\n",
       "  0.006636049522237348,\n",
       "  0.006590127966824614,\n",
       "  0.0065486472641965296,\n",
       "  0.006499512667803236,\n",
       "  0.006458675058401067],\n",
       " 'val_loss': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork('mse')\n",
    "model.add_layer(DenseLayer(output_size=128, activation=relu, init=\"Xavier\"))\n",
    "model.add_layer(DenseLayer(output_size=64, activation=relu, init=\"Xavier\"))\n",
    "model.add_layer(DenseLayer(output_size=10, activation=sigmoid, init=\"Xavier\"))\n",
    "\n",
    "model.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.05,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9584\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "pred_classes = np.argmax(predictions, axis=1)\n",
    "accuracy = accuracy_score(pred_classes, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of depth (Number of layers) and Width (Number of neurons per layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed Depth\n",
    "- Number of hidden layers : 2\n",
    "- Test 1 : 64 neurons per layer\n",
    "- Test 2 : 128 neurons per layer\n",
    "- Test 3 : 256 neurons per layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================>] 100%\n",
      "Epoch 1/100 - 1.70s - loss: 0.2490 - val_loss: 0.2324\n",
      "[=================================================>] 100%\n",
      "Epoch 2/100 - 1.71s - loss: 0.2177 - val_loss: 0.2028\n",
      "[=================================================>] 100%\n",
      "Epoch 3/100 - 1.62s - loss: 0.1879 - val_loss: 0.1729\n",
      "[=================================================>] 100%\n",
      "Epoch 4/100 - 1.53s - loss: 0.1591 - val_loss: 0.1461\n",
      "[=================================================>] 100%\n",
      "Epoch 5/100 - 1.98s - loss: 0.1355 - val_loss: 0.1260\n",
      "[=================================================>] 100%\n",
      "Epoch 6/100 - 2.10s - loss: 0.1189 - val_loss: 0.1128\n",
      "[=================================================>] 100%\n",
      "Epoch 7/100 - 1.69s - loss: 0.1085 - val_loss: 0.1048\n",
      "[=================================================>] 100%\n",
      "Epoch 8/100 - 1.91s - loss: 0.1022 - val_loss: 0.1000\n",
      "[=================================================>] 100%\n",
      "Epoch 9/100 - 1.70s - loss: 0.0985 - val_loss: 0.0971\n",
      "[=================================================>] 100%\n",
      "Epoch 10/100 - 1.80s - loss: 0.0961 - val_loss: 0.0953\n",
      "[=================================================>] 100%\n",
      "Epoch 11/100 - 1.57s - loss: 0.0946 - val_loss: 0.0940\n",
      "[=================================================>] 100%\n",
      "Epoch 12/100 - 1.51s - loss: 0.0936 - val_loss: 0.0932\n",
      "[=================================================>] 100%\n",
      "Epoch 13/100 - 1.53s - loss: 0.0929 - val_loss: 0.0926\n",
      "[=================================================>] 100%\n",
      "Epoch 14/100 - 1.47s - loss: 0.0924 - val_loss: 0.0922\n",
      "[=================================================>] 100%\n",
      "Epoch 15/100 - 1.55s - loss: 0.0921 - val_loss: 0.0919\n",
      "[=================================================>] 100%\n",
      "Epoch 16/100 - 1.63s - loss: 0.0918 - val_loss: 0.0916\n",
      "[=================================================>] 100%\n",
      "Epoch 17/100 - 1.61s - loss: 0.0915 - val_loss: 0.0914\n",
      "[=================================================>] 100%\n",
      "Epoch 18/100 - 1.49s - loss: 0.0914 - val_loss: 0.0913\n",
      "[=================================================>] 100%\n",
      "Epoch 19/100 - 1.55s - loss: 0.0912 - val_loss: 0.0911\n",
      "[=================================================>] 100%\n",
      "Epoch 20/100 - 1.56s - loss: 0.0911 - val_loss: 0.0910\n",
      "[=================================================>] 100%\n",
      "Epoch 21/100 - 1.81s - loss: 0.0910 - val_loss: 0.0909\n",
      "[=================================================>] 100%\n",
      "Epoch 22/100 - 1.62s - loss: 0.0909 - val_loss: 0.0908\n",
      "[=================================================>] 100%\n",
      "Epoch 23/100 - 1.88s - loss: 0.0908 - val_loss: 0.0907\n",
      "[=================================================>] 100%\n",
      "Epoch 24/100 - 1.97s - loss: 0.0907 - val_loss: 0.0906\n",
      "[=================================================>] 100%\n",
      "Epoch 25/100 - 1.75s - loss: 0.0906 - val_loss: 0.0906\n",
      "[=================================================>] 100%\n",
      "Epoch 26/100 - 1.60s - loss: 0.0905 - val_loss: 0.0905\n",
      "[=================================================>] 100%\n",
      "Epoch 27/100 - 1.49s - loss: 0.0905 - val_loss: 0.0904\n",
      "[=================================================>] 100%\n",
      "Epoch 28/100 - 1.56s - loss: 0.0904 - val_loss: 0.0903\n",
      "[=================================================>] 100%\n",
      "Epoch 29/100 - 1.43s - loss: 0.0903 - val_loss: 0.0903\n",
      "[=================================================>] 100%\n",
      "Epoch 30/100 - 1.76s - loss: 0.0902 - val_loss: 0.0902\n",
      "[=================================================>] 100%\n",
      "Epoch 31/100 - 1.65s - loss: 0.0902 - val_loss: 0.0901\n",
      "[=================================================>] 100%\n",
      "Epoch 32/100 - 1.76s - loss: 0.0901 - val_loss: 0.0901\n",
      "[=================================================>] 100%\n",
      "Epoch 33/100 - 1.64s - loss: 0.0900 - val_loss: 0.0900\n",
      "[=================================================>] 100%\n",
      "Epoch 34/100 - 1.77s - loss: 0.0900 - val_loss: 0.0899\n",
      "[=================================================>] 100%\n",
      "Epoch 35/100 - 1.53s - loss: 0.0899 - val_loss: 0.0898\n",
      "[=================================================>] 100%\n",
      "Epoch 36/100 - 1.50s - loss: 0.0898 - val_loss: 0.0898\n",
      "[=================================================>] 100%\n",
      "Epoch 37/100 - 1.53s - loss: 0.0897 - val_loss: 0.0897\n",
      "[=================================================>] 100%\n",
      "Epoch 38/100 - 1.60s - loss: 0.0897 - val_loss: 0.0896\n",
      "[=================================================>] 100%\n",
      "Epoch 39/100 - 1.55s - loss: 0.0896 - val_loss: 0.0896\n",
      "[=================================================>] 100%\n",
      "Epoch 40/100 - 1.43s - loss: 0.0895 - val_loss: 0.0895\n",
      "[=================================================>] 100%\n",
      "Epoch 41/100 - 1.48s - loss: 0.0895 - val_loss: 0.0894\n",
      "[=================================================>] 100%\n",
      "Epoch 42/100 - 1.68s - loss: 0.0894 - val_loss: 0.0893\n",
      "[=================================================>] 100%\n",
      "Epoch 43/100 - 1.52s - loss: 0.0893 - val_loss: 0.0893\n",
      "[=================================================>] 100%\n",
      "Epoch 44/100 - 1.91s - loss: 0.0892 - val_loss: 0.0892\n",
      "[=================================================>] 100%\n",
      "Epoch 45/100 - 1.61s - loss: 0.0891 - val_loss: 0.0891\n",
      "[=================================================>] 100%\n",
      "Epoch 46/100 - 1.47s - loss: 0.0891 - val_loss: 0.0890\n",
      "[=================================================>] 100%\n",
      "Epoch 47/100 - 1.47s - loss: 0.0890 - val_loss: 0.0889\n",
      "[=================================================>] 100%\n",
      "Epoch 48/100 - 1.47s - loss: 0.0889 - val_loss: 0.0888\n",
      "[=================================================>] 100%\n",
      "Epoch 49/100 - 1.47s - loss: 0.0888 - val_loss: 0.0888\n",
      "[=================================================>] 100%\n",
      "Epoch 50/100 - 1.53s - loss: 0.0887 - val_loss: 0.0887\n",
      "[=================================================>] 100%\n",
      "Epoch 51/100 - 1.42s - loss: 0.0886 - val_loss: 0.0886\n",
      "[=================================================>] 100%\n",
      "Epoch 52/100 - 1.52s - loss: 0.0885 - val_loss: 0.0885\n",
      "[=================================================>] 100%\n",
      "Epoch 53/100 - 1.55s - loss: 0.0884 - val_loss: 0.0884\n",
      "[=================================================>] 100%\n",
      "Epoch 54/100 - 1.56s - loss: 0.0883 - val_loss: 0.0883\n",
      "[=================================================>] 100%\n",
      "Epoch 55/100 - 1.57s - loss: 0.0883 - val_loss: 0.0882\n",
      "[=================================================>] 100%\n",
      "Epoch 56/100 - 1.58s - loss: 0.0882 - val_loss: 0.0881\n",
      "[=================================================>] 100%\n",
      "Epoch 57/100 - 1.45s - loss: 0.0880 - val_loss: 0.0880\n",
      "[=================================================>] 100%\n",
      "Epoch 58/100 - 1.50s - loss: 0.0879 - val_loss: 0.0879\n",
      "[=================================================>] 100%\n",
      "Epoch 59/100 - 1.53s - loss: 0.0878 - val_loss: 0.0878\n",
      "[=================================================>] 100%\n",
      "Epoch 60/100 - 1.49s - loss: 0.0877 - val_loss: 0.0877\n",
      "[=================================================>] 100%\n",
      "Epoch 61/100 - 1.44s - loss: 0.0876 - val_loss: 0.0875\n",
      "[=================================================>] 100%\n",
      "Epoch 62/100 - 1.46s - loss: 0.0875 - val_loss: 0.0874\n",
      "[=================================================>] 100%\n",
      "Epoch 63/100 - 1.58s - loss: 0.0874 - val_loss: 0.0873\n",
      "[=================================================>] 100%\n",
      "Epoch 64/100 - 1.50s - loss: 0.0873 - val_loss: 0.0872\n",
      "[=================================================>] 100%\n",
      "Epoch 65/100 - 1.56s - loss: 0.0871 - val_loss: 0.0871\n",
      "[=================================================>] 100%\n",
      "Epoch 66/100 - 1.58s - loss: 0.0870 - val_loss: 0.0869\n",
      "[=================================================>] 100%\n",
      "Epoch 67/100 - 1.40s - loss: 0.0869 - val_loss: 0.0868\n",
      "[=================================================>] 100%\n",
      "Epoch 68/100 - 1.61s - loss: 0.0867 - val_loss: 0.0867\n",
      "[=================================================>] 100%\n",
      "Epoch 69/100 - 1.63s - loss: 0.0866 - val_loss: 0.0865\n",
      "[=================================================>] 100%\n",
      "Epoch 70/100 - 1.41s - loss: 0.0865 - val_loss: 0.0864\n",
      "[=================================================>] 100%\n",
      "Epoch 71/100 - 1.56s - loss: 0.0863 - val_loss: 0.0862\n",
      "[=================================================>] 100%\n",
      "Epoch 72/100 - 1.65s - loss: 0.0862 - val_loss: 0.0861\n",
      "[=================================================>] 100%\n",
      "Epoch 73/100 - 1.46s - loss: 0.0860 - val_loss: 0.0859\n",
      "[=================================================>] 100%\n",
      "Epoch 74/100 - 1.40s - loss: 0.0859 - val_loss: 0.0858\n",
      "[=================================================>] 100%\n",
      "Epoch 75/100 - 1.53s - loss: 0.0857 - val_loss: 0.0856\n",
      "[=================================================>] 100%\n",
      "Epoch 76/100 - 1.47s - loss: 0.0856 - val_loss: 0.0855\n",
      "[=================================================>] 100%\n",
      "Epoch 77/100 - 1.39s - loss: 0.0854 - val_loss: 0.0853\n",
      "[=================================================>] 100%\n",
      "Epoch 78/100 - 1.51s - loss: 0.0852 - val_loss: 0.0851\n",
      "[=================================================>] 100%\n",
      "Epoch 79/100 - 1.53s - loss: 0.0851 - val_loss: 0.0849\n",
      "[=================================================>] 100%\n",
      "Epoch 80/100 - 1.38s - loss: 0.0849 - val_loss: 0.0848\n",
      "[=================================================>] 100%\n",
      "Epoch 81/100 - 1.59s - loss: 0.0847 - val_loss: 0.0846\n",
      "[=================================================>] 100%\n",
      "Epoch 82/100 - 1.72s - loss: 0.0845 - val_loss: 0.0844\n",
      "[=================================================>] 100%\n",
      "Epoch 83/100 - 1.70s - loss: 0.0843 - val_loss: 0.0842\n",
      "[=================================================>] 100%\n",
      "Epoch 84/100 - 1.49s - loss: 0.0841 - val_loss: 0.0840\n",
      "[=================================================>] 100%\n",
      "Epoch 85/100 - 1.62s - loss: 0.0839 - val_loss: 0.0838\n",
      "[=================================================>] 100%\n",
      "Epoch 86/100 - 1.55s - loss: 0.0837 - val_loss: 0.0836\n",
      "[=================================================>] 100%\n",
      "Epoch 87/100 - 1.58s - loss: 0.0835 - val_loss: 0.0834\n",
      "[=================================================>] 100%\n",
      "Epoch 88/100 - 1.52s - loss: 0.0833 - val_loss: 0.0832\n",
      "[=================================================>] 100%\n",
      "Epoch 89/100 - 1.51s - loss: 0.0831 - val_loss: 0.0830\n",
      "[=================================================>] 100%\n",
      "Epoch 90/100 - 1.50s - loss: 0.0829 - val_loss: 0.0827\n",
      "[=================================================>] 100%\n",
      "Epoch 91/100 - 1.59s - loss: 0.0826 - val_loss: 0.0825\n",
      "[=================================================>] 100%\n",
      "Epoch 92/100 - 1.53s - loss: 0.0824 - val_loss: 0.0823\n",
      "[=================================================>] 100%\n",
      "Epoch 93/100 - 1.56s - loss: 0.0822 - val_loss: 0.0820\n",
      "[=================================================>] 100%\n",
      "Epoch 94/100 - 1.44s - loss: 0.0819 - val_loss: 0.0818\n",
      "[=================================================>] 100%\n",
      "Epoch 95/100 - 1.55s - loss: 0.0817 - val_loss: 0.0816\n",
      "[=================================================>] 100%\n",
      "Epoch 96/100 - 1.51s - loss: 0.0815 - val_loss: 0.0813\n",
      "[=================================================>] 100%\n",
      "Epoch 97/100 - 1.48s - loss: 0.0812 - val_loss: 0.0810\n",
      "[=================================================>] 100%\n",
      "Epoch 98/100 - 1.59s - loss: 0.0809 - val_loss: 0.0808\n",
      "[=================================================>] 100%\n",
      "Epoch 99/100 - 1.56s - loss: 0.0807 - val_loss: 0.0805\n",
      "[=================================================>] 100%\n",
      "Epoch 100/100 - 1.53s - loss: 0.0804 - val_loss: 0.0803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.24896870999864829,\n",
       "  0.21772392982372143,\n",
       "  0.1878529883690506,\n",
       "  0.15913406169171143,\n",
       "  0.13547849554725733,\n",
       "  0.11888395854282767,\n",
       "  0.10846971771868423,\n",
       "  0.10221966329036776,\n",
       "  0.09845411889933237,\n",
       "  0.09612035797413274,\n",
       "  0.09461976627607036,\n",
       "  0.09361719290638693,\n",
       "  0.09292223669802564,\n",
       "  0.09242344049592666,\n",
       "  0.09205330529661417,\n",
       "  0.0917698732809284,\n",
       "  0.09154618515040751,\n",
       "  0.09136444076693685,\n",
       "  0.09121272032420445,\n",
       "  0.09108276012158575,\n",
       "  0.09096874825612844,\n",
       "  0.090866563289696,\n",
       "  0.0907732190541417,\n",
       "  0.09068650264306288,\n",
       "  0.09060473894455172,\n",
       "  0.09052672685408471,\n",
       "  0.09045152268771581,\n",
       "  0.09037839559564889,\n",
       "  0.09030676406045215,\n",
       "  0.09023614192226592,\n",
       "  0.09016616182642946,\n",
       "  0.09009653175515227,\n",
       "  0.09002701615739579,\n",
       "  0.08995740807194884,\n",
       "  0.08988751027646762,\n",
       "  0.08981716356881375,\n",
       "  0.0897462286851297,\n",
       "  0.0896745669906275,\n",
       "  0.08960207383714722,\n",
       "  0.08952864436498795,\n",
       "  0.08945421173968954,\n",
       "  0.08937866051272636,\n",
       "  0.08930186656640292,\n",
       "  0.08922373111705265,\n",
       "  0.0891441705885225,\n",
       "  0.08906306442489904,\n",
       "  0.08898033932809185,\n",
       "  0.08889591168356167,\n",
       "  0.08880971153115627,\n",
       "  0.08872166403281294,\n",
       "  0.08863172372453822,\n",
       "  0.08853976001873276,\n",
       "  0.08844573376091991,\n",
       "  0.0883495966796624,\n",
       "  0.08825125345284952,\n",
       "  0.08815063131780276,\n",
       "  0.08804769798636379,\n",
       "  0.08794236679871303,\n",
       "  0.08783462128571945,\n",
       "  0.08772439794829012,\n",
       "  0.08761157771240556,\n",
       "  0.08749616145266935,\n",
       "  0.08737809471295384,\n",
       "  0.08725727136934447,\n",
       "  0.08713361816669374,\n",
       "  0.08700703215789486,\n",
       "  0.08687740781690453,\n",
       "  0.08674468418743997,\n",
       "  0.08660873344590234,\n",
       "  0.08646946711581584,\n",
       "  0.08632681610398195,\n",
       "  0.08618066936170582,\n",
       "  0.08603087989605133,\n",
       "  0.08587742815681264,\n",
       "  0.08572031326571923,\n",
       "  0.0855594509832944,\n",
       "  0.0853948275961087,\n",
       "  0.08522629167899076,\n",
       "  0.08505372292156418,\n",
       "  0.08487711932635454,\n",
       "  0.08469632266231011,\n",
       "  0.08451129774788134,\n",
       "  0.08432200977965114,\n",
       "  0.08412837343624598,\n",
       "  0.08393033149141123,\n",
       "  0.08372779808534345,\n",
       "  0.08352076776548331,\n",
       "  0.08330924554973057,\n",
       "  0.08309321961439703,\n",
       "  0.08287268972047929,\n",
       "  0.08264767140664708,\n",
       "  0.08241806537429544,\n",
       "  0.08218390789459888,\n",
       "  0.08194526188353168,\n",
       "  0.08170214355167871,\n",
       "  0.08145452671596703,\n",
       "  0.08120248705797177,\n",
       "  0.08094610324058925,\n",
       "  0.08068539983547594,\n",
       "  0.08042047768106723],\n",
       " 'val_loss': [0.23243722846093587,\n",
       "  0.20282613009590397,\n",
       "  0.17290189366180886,\n",
       "  0.14614596709184408,\n",
       "  0.1259745253593214,\n",
       "  0.11279057342300836,\n",
       "  0.10479486954305504,\n",
       "  0.10001431986561567,\n",
       "  0.09709691310253098,\n",
       "  0.09525377987765801,\n",
       "  0.09404414028061918,\n",
       "  0.09321963715012976,\n",
       "  0.09263729613768248,\n",
       "  0.09221193568096618,\n",
       "  0.09189104012084642,\n",
       "  0.09164138716002118,\n",
       "  0.09144132060565485,\n",
       "  0.09127643455045639,\n",
       "  0.09113687657100632,\n",
       "  0.09101577920154685,\n",
       "  0.09090830386761245,\n",
       "  0.09081097594574426,\n",
       "  0.09072122261231787,\n",
       "  0.0906371620887999,\n",
       "  0.09055737403063396,\n",
       "  0.0904808251354736,\n",
       "  0.09040667668906209,\n",
       "  0.09033429199208397,\n",
       "  0.09026314421084213,\n",
       "  0.0901928068515804,\n",
       "  0.09012294897514252,\n",
       "  0.09005329174266581,\n",
       "  0.08998364060208275,\n",
       "  0.08991379488254872,\n",
       "  0.08984355726898666,\n",
       "  0.08977281092959988,\n",
       "  0.08970140103358201,\n",
       "  0.0896292367348274,\n",
       "  0.08955620276325293,\n",
       "  0.08948218278019586,\n",
       "  0.0894071030578969,\n",
       "  0.08933082047105123,\n",
       "  0.08925323611763107,\n",
       "  0.08917428330259891,\n",
       "  0.08909383329948571,\n",
       "  0.0890118228119953,\n",
       "  0.08892815333842434,\n",
       "  0.08884278599010831,\n",
       "  0.0887555969449068,\n",
       "  0.08866651365588024,\n",
       "  0.0885753940317695,\n",
       "  0.08848218837730142,\n",
       "  0.08838689579053065,\n",
       "  0.08828948155235708,\n",
       "  0.08818985895317225,\n",
       "  0.08808797367936685,\n",
       "  0.08798376190742452,\n",
       "  0.08787710691399508,\n",
       "  0.08776796085845959,\n",
       "  0.08765623835410385,\n",
       "  0.08754187341445833,\n",
       "  0.08742481967278365,\n",
       "  0.08730507942983577,\n",
       "  0.08718260950186385,\n",
       "  0.08705719083876427,\n",
       "  0.08692894021079234,\n",
       "  0.0867976812905464,\n",
       "  0.08666319941356398,\n",
       "  0.08652542315256133,\n",
       "  0.08638431308950603,\n",
       "  0.08623976645433126,\n",
       "  0.08609168203410547,\n",
       "  0.08593992162999048,\n",
       "  0.0857844654388672,\n",
       "  0.08562531833740017,\n",
       "  0.08546236659238023,\n",
       "  0.08529559381738774,\n",
       "  0.08512478972941986,\n",
       "  0.08494992272428949,\n",
       "  0.08477092460656567,\n",
       "  0.08458773359171308,\n",
       "  0.08440027751674994,\n",
       "  0.08420849091165246,\n",
       "  0.08401232037389546,\n",
       "  0.08381179774896655,\n",
       "  0.08360666019404224,\n",
       "  0.08339697678150323,\n",
       "  0.08318269282257157,\n",
       "  0.08296381297942447,\n",
       "  0.08274034429918574,\n",
       "  0.08251227022378625,\n",
       "  0.08227954338034035,\n",
       "  0.08204227621506235,\n",
       "  0.08180055005424133,\n",
       "  0.08155435254313817,\n",
       "  0.08130368634010827,\n",
       "  0.08104860767811414,\n",
       "  0.08078922738385078,\n",
       "  0.080525604757252,\n",
       "  0.08025788058797835]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test_1_fixed_depth = NeuralNetwork('mse')\n",
    "model_test_1_fixed_depth.add_layer(DenseLayer(output_size=128, activation=ReLu(), init=\"Xavier\"))\n",
    "model_test_1_fixed_depth.add_layer(DenseLayer(output_size=64, activation=ReLu(), init=\"Xavier\"))\n",
    "model_test_1_fixed_depth.add_layer(DenseLayer(output_size=10, activation=Sigmoid(), init=\"Xavier\"))\n",
    "\n",
    "model_test_1_fixed_depth.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.05,\n",
    "    optimizer=\"gradient_descent\",\n",
    "    isOne_hot=True,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5054\n"
     ]
    }
   ],
   "source": [
    "predictions_test_1_fixed_depth = model_test_1_fixed_depth.predict(X_test)\n",
    "pred_classes_test_1_fixed_depth = np.argmax(predictions_test_1_fixed_depth, axis=1)\n",
    "accuracy_test_1_fixed_depth = accuracy_score(pred_classes_test_1_fixed_depth, y_test)\n",
    "print(\"Test Accuracy:\", accuracy_test_1_fixed_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare model with sklearn MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37724840\n",
      "Iteration 2, loss = 0.15004672\n",
      "Iteration 3, loss = 0.10562821\n",
      "Iteration 4, loss = 0.08064147\n",
      "Iteration 5, loss = 0.06428497\n",
      "Iteration 6, loss = 0.05121985\n",
      "Iteration 7, loss = 0.04219482\n",
      "Iteration 8, loss = 0.03723277\n",
      "Iteration 9, loss = 0.02986735\n",
      "Iteration 10, loss = 0.02325205\n",
      "Iteration 11, loss = 0.02124234\n",
      "Iteration 12, loss = 0.01549812\n",
      "Iteration 13, loss = 0.01433403\n",
      "Iteration 14, loss = 0.01377085\n",
      "Iteration 15, loss = 0.00927998\n",
      "Iteration 16, loss = 0.00973711\n",
      "Iteration 17, loss = 0.01097088\n",
      "Iteration 18, loss = 0.00895768\n",
      "Iteration 19, loss = 0.00620646\n",
      "Iteration 20, loss = 0.00376964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=20, random_state=1,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=20, random_state=1,\n",
       "              verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=20, random_state=1,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(128, 64),activation='relu', \n",
    "                    solver='adam', max_iter=20, random_state=1, verbose=True)\n",
    "\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier Test Accuracy: 0.9790\n"
     ]
    }
   ],
   "source": [
    "y_pred_mlp = mlp.predict(X_test)\n",
    "acc_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "print(f\"MLPClassifier Test Accuracy: {acc_mlp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       986\n",
      "           1       0.99      0.99      0.99      1125\n",
      "           2       0.97      0.98      0.98       999\n",
      "           3       0.99      0.95      0.97      1020\n",
      "           4       0.98      0.98      0.98       975\n",
      "           5       0.96      0.98      0.97       902\n",
      "           6       0.99      0.99      0.99       982\n",
      "           7       0.98      0.98      0.98      1042\n",
      "           8       0.98      0.97      0.97       975\n",
      "           9       0.95      0.99      0.97       994\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
