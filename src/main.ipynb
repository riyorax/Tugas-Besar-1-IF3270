{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF3270 Pembelajaran Mesin | Tugas Besar - Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Members:\n",
    "- Maximilian Sulistiyo (13522061)\n",
    "- Marvel Pangondian (13522075)\n",
    "- Abdullah Mubarak (13522101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we implement a custom built Feedforward Neural Network with no high-level libraries. The goal in this project is to be able to create a custom FFNN that is able to specify the type of activation function on each layer, the type of loss function, and how many neurons in each layer. We will also compare our algorithm with other built in algorithm (the sklearn MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ann import NeuralNetwork, one_hot, get_accuracy\n",
    "from dense_layer import DenseLayer\n",
    "from activations import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy dataset and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original = X.copy()\n",
    "X_original = X_original/255.0\n",
    "y_original = y.copy()\n",
    "y_original = y_original.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_original, y_original, train_size=60000, test_size=10000, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_oh = one_hot(y_train)\n",
    "y_test_oh = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================>] 100%\n",
      "Epoch 1/100 - 1.63s - loss: 0.2185 - val_loss: 0.2062\n",
      "[=================================================>] 100%\n",
      "Epoch 2/100 - 1.91s - loss: 0.1924 - val_loss: 0.1782\n",
      "[=================================================>] 100%\n",
      "Epoch 3/100 - 1.71s - loss: 0.1640 - val_loss: 0.1505\n",
      "[=================================================>] 100%\n",
      "Epoch 4/100 - 1.61s - loss: 0.1387 - val_loss: 0.1283\n",
      "[=================================================>] 100%\n",
      "Epoch 5/100 - 1.56s - loss: 0.1203 - val_loss: 0.1135\n",
      "[=================================================>] 100%\n",
      "Epoch 6/100 - 1.65s - loss: 0.1087 - val_loss: 0.1047\n",
      "[=================================================>] 100%\n",
      "Epoch 7/100 - 1.57s - loss: 0.1019 - val_loss: 0.0996\n",
      "[=================================================>] 100%\n",
      "Epoch 8/100 - 1.54s - loss: 0.0979 - val_loss: 0.0965\n",
      "[=================================================>] 100%\n",
      "Epoch 9/100 - 1.47s - loss: 0.0954 - val_loss: 0.0946\n",
      "[=================================================>] 100%\n",
      "Epoch 10/100 - 1.52s - loss: 0.0939 - val_loss: 0.0933\n",
      "[=================================================>] 100%\n",
      "Epoch 11/100 - 1.56s - loss: 0.0928 - val_loss: 0.0924\n",
      "[=================================================>] 100%\n",
      "Epoch 12/100 - 1.51s - loss: 0.0921 - val_loss: 0.0918\n",
      "[=================================================>] 100%\n",
      "Epoch 13/100 - 1.50s - loss: 0.0916 - val_loss: 0.0913\n",
      "[=================================================>] 100%\n",
      "Epoch 14/100 - 1.60s - loss: 0.0911 - val_loss: 0.0910\n",
      "[=================================================>] 100%\n",
      "Epoch 15/100 - 1.53s - loss: 0.0908 - val_loss: 0.0907\n",
      "[=================================================>] 100%\n",
      "Epoch 16/100 - 1.62s - loss: 0.0905 - val_loss: 0.0904\n",
      "[=================================================>] 100%\n",
      "Epoch 17/100 - 1.55s - loss: 0.0903 - val_loss: 0.0902\n",
      "[=================================================>] 100%\n",
      "Epoch 18/100 - 1.57s - loss: 0.0901 - val_loss: 0.0900\n",
      "[=================================================>] 100%\n",
      "Epoch 19/100 - 1.59s - loss: 0.0899 - val_loss: 0.0898\n",
      "[=================================================>] 100%\n",
      "Epoch 20/100 - 1.59s - loss: 0.0897 - val_loss: 0.0896\n",
      "[=================================================>] 100%\n",
      "Epoch 21/100 - 1.68s - loss: 0.0895 - val_loss: 0.0894\n",
      "[=================================================>] 100%\n",
      "Epoch 22/100 - 1.55s - loss: 0.0893 - val_loss: 0.0893\n",
      "[=================================================>] 100%\n",
      "Epoch 23/100 - 1.51s - loss: 0.0892 - val_loss: 0.0891\n",
      "[=================================================>] 100%\n",
      "Epoch 24/100 - 1.61s - loss: 0.0890 - val_loss: 0.0890\n",
      "[=================================================>] 100%\n",
      "Epoch 25/100 - 1.48s - loss: 0.0889 - val_loss: 0.0888\n",
      "[=================================================>] 100%\n",
      "Epoch 26/100 - 1.81s - loss: 0.0887 - val_loss: 0.0886\n",
      "[=================================================>] 100%\n",
      "Epoch 27/100 - 1.80s - loss: 0.0885 - val_loss: 0.0885\n",
      "[=================================================>] 100%\n",
      "Epoch 28/100 - 1.79s - loss: 0.0884 - val_loss: 0.0883\n",
      "[=================================================>] 100%\n",
      "Epoch 29/100 - 1.93s - loss: 0.0882 - val_loss: 0.0881\n",
      "[=================================================>] 100%\n",
      "Epoch 30/100 - 1.90s - loss: 0.0880 - val_loss: 0.0880\n",
      "[=================================================>] 100%\n",
      "Epoch 31/100 - 1.73s - loss: 0.0879 - val_loss: 0.0878\n",
      "[=================================================>] 100%\n",
      "Epoch 32/100 - 1.67s - loss: 0.0877 - val_loss: 0.0876\n",
      "[=================================================>] 100%\n",
      "Epoch 33/100 - 1.75s - loss: 0.0875 - val_loss: 0.0874\n",
      "[=================================================>] 100%\n",
      "Epoch 34/100 - 1.97s - loss: 0.0873 - val_loss: 0.0873\n",
      "[=================================================>] 100%\n",
      "Epoch 35/100 - 1.98s - loss: 0.0872 - val_loss: 0.0871\n",
      "[=================================================>] 100%\n",
      "Epoch 36/100 - 2.13s - loss: 0.0870 - val_loss: 0.0869\n",
      "[=================================================>] 100%\n",
      "Epoch 37/100 - 1.69s - loss: 0.0868 - val_loss: 0.0867\n",
      "[=================================================>] 100%\n",
      "Epoch 38/100 - 1.58s - loss: 0.0866 - val_loss: 0.0865\n",
      "[=================================================>] 100%\n",
      "Epoch 39/100 - 1.53s - loss: 0.0864 - val_loss: 0.0863\n",
      "[=================================================>] 100%\n",
      "Epoch 40/100 - 1.62s - loss: 0.0862 - val_loss: 0.0861\n",
      "[=================================================>] 100%\n",
      "Epoch 41/100 - 1.54s - loss: 0.0860 - val_loss: 0.0859\n",
      "[=================================================>] 100%\n",
      "Epoch 42/100 - 1.55s - loss: 0.0858 - val_loss: 0.0857\n",
      "[=================================================>] 100%\n",
      "Epoch 43/100 - 1.61s - loss: 0.0855 - val_loss: 0.0854\n",
      "[=================================================>] 100%\n",
      "Epoch 44/100 - 1.56s - loss: 0.0853 - val_loss: 0.0852\n",
      "[=================================================>] 100%\n",
      "Epoch 45/100 - 1.50s - loss: 0.0851 - val_loss: 0.0850\n",
      "[=================================================>] 100%\n",
      "Epoch 46/100 - 1.50s - loss: 0.0848 - val_loss: 0.0847\n",
      "[=================================================>] 100%\n",
      "Epoch 47/100 - 2.10s - loss: 0.0846 - val_loss: 0.0845\n",
      "[=================================================>] 100%\n",
      "Epoch 48/100 - 1.81s - loss: 0.0844 - val_loss: 0.0843\n",
      "[=================================================>] 100%\n",
      "Epoch 49/100 - 1.77s - loss: 0.0841 - val_loss: 0.0840\n",
      "[=================================================>] 100%\n",
      "Epoch 50/100 - 1.76s - loss: 0.0839 - val_loss: 0.0837\n",
      "[=================================================>] 100%\n",
      "Epoch 51/100 - 1.69s - loss: 0.0836 - val_loss: 0.0835\n",
      "[=================================================>] 100%\n",
      "Epoch 52/100 - 2.00s - loss: 0.0833 - val_loss: 0.0832\n",
      "[=================================================>] 100%\n",
      "Epoch 53/100 - 1.95s - loss: 0.0830 - val_loss: 0.0829\n",
      "[=================================================>] 100%\n",
      "Epoch 54/100 - 1.88s - loss: 0.0828 - val_loss: 0.0826\n",
      "[=================================================>] 100%\n",
      "Epoch 55/100 - 1.80s - loss: 0.0825 - val_loss: 0.0824\n",
      "[=================================================>] 100%\n",
      "Epoch 56/100 - 1.84s - loss: 0.0822 - val_loss: 0.0821\n",
      "[=================================================>] 100%\n",
      "Epoch 57/100 - 1.87s - loss: 0.0819 - val_loss: 0.0818\n",
      "[=================================================>] 100%\n",
      "Epoch 58/100 - 1.85s - loss: 0.0816 - val_loss: 0.0815\n",
      "[=================================================>] 100%\n",
      "Epoch 59/100 - 1.84s - loss: 0.0813 - val_loss: 0.0812\n",
      "[=================================================>] 100%\n",
      "Epoch 60/100 - 1.87s - loss: 0.0810 - val_loss: 0.0808\n",
      "[=================================================>] 100%\n",
      "Epoch 61/100 - 1.82s - loss: 0.0807 - val_loss: 0.0805\n",
      "[=================================================>] 100%\n",
      "Epoch 62/100 - 2.50s - loss: 0.0803 - val_loss: 0.0802\n",
      "[=================================================>] 100%\n",
      "Epoch 63/100 - 1.56s - loss: 0.0800 - val_loss: 0.0799\n",
      "[=================================================>] 100%\n",
      "Epoch 64/100 - 1.82s - loss: 0.0797 - val_loss: 0.0795\n",
      "[=================================================>] 100%\n",
      "Epoch 65/100 - 1.83s - loss: 0.0793 - val_loss: 0.0792\n",
      "[=================================================>] 100%\n",
      "Epoch 66/100 - 1.69s - loss: 0.0790 - val_loss: 0.0788\n",
      "[=================================================>] 100%\n",
      "Epoch 67/100 - 1.69s - loss: 0.0787 - val_loss: 0.0785\n",
      "[=================================================>] 100%\n",
      "Epoch 68/100 - 1.79s - loss: 0.0783 - val_loss: 0.0781\n",
      "[=================================================>] 100%\n",
      "Epoch 69/100 - 1.81s - loss: 0.0779 - val_loss: 0.0778\n",
      "[=================================================>] 100%\n",
      "Epoch 70/100 - 1.86s - loss: 0.0776 - val_loss: 0.0774\n",
      "[=================================================>] 100%\n",
      "Epoch 71/100 - 2.18s - loss: 0.0772 - val_loss: 0.0770\n",
      "[=================================================>] 100%\n",
      "Epoch 72/100 - 1.73s - loss: 0.0769 - val_loss: 0.0767\n",
      "[=================================================>] 100%\n",
      "Epoch 73/100 - 1.93s - loss: 0.0765 - val_loss: 0.0763\n",
      "[=================================================>] 100%\n",
      "Epoch 74/100 - 1.96s - loss: 0.0761 - val_loss: 0.0759\n",
      "[=================================================>] 100%\n",
      "Epoch 75/100 - 1.94s - loss: 0.0758 - val_loss: 0.0756\n",
      "[=================================================>] 100%\n",
      "Epoch 76/100 - 1.62s - loss: 0.0754 - val_loss: 0.0752\n",
      "[=================================================>] 100%\n",
      "Epoch 77/100 - 1.63s - loss: 0.0750 - val_loss: 0.0748\n",
      "[=================================================>] 100%\n",
      "Epoch 78/100 - 1.67s - loss: 0.0746 - val_loss: 0.0744\n",
      "[=================================================>] 100%\n",
      "Epoch 79/100 - 1.56s - loss: 0.0742 - val_loss: 0.0740\n",
      "[=================================================>] 100%\n",
      "Epoch 80/100 - 1.57s - loss: 0.0739 - val_loss: 0.0737\n",
      "[=================================================>] 100%\n",
      "Epoch 81/100 - 1.66s - loss: 0.0735 - val_loss: 0.0733\n",
      "[=================================================>] 100%\n",
      "Epoch 82/100 - 1.80s - loss: 0.0731 - val_loss: 0.0729\n",
      "[=================================================>] 100%\n",
      "Epoch 83/100 - 1.70s - loss: 0.0727 - val_loss: 0.0725\n",
      "[=================================================>] 100%\n",
      "Epoch 84/100 - 1.76s - loss: 0.0723 - val_loss: 0.0721\n",
      "[=================================================>] 100%\n",
      "Epoch 85/100 - 1.65s - loss: 0.0720 - val_loss: 0.0717\n",
      "[=================================================>] 100%\n",
      "Epoch 86/100 - 1.60s - loss: 0.0716 - val_loss: 0.0714\n",
      "[=================================================>] 100%\n",
      "Epoch 87/100 - 1.88s - loss: 0.0712 - val_loss: 0.0710\n",
      "[=================================================>] 100%\n",
      "Epoch 88/100 - 1.50s - loss: 0.0708 - val_loss: 0.0706\n",
      "[=================================================>] 100%\n",
      "Epoch 89/100 - 1.51s - loss: 0.0705 - val_loss: 0.0702\n",
      "[=================================================>] 100%\n",
      "Epoch 90/100 - 1.55s - loss: 0.0701 - val_loss: 0.0699\n",
      "[=================================================>] 100%\n",
      "Epoch 91/100 - 1.51s - loss: 0.0697 - val_loss: 0.0695\n",
      "[=================================================>] 100%\n",
      "Epoch 92/100 - 1.47s - loss: 0.0693 - val_loss: 0.0691\n",
      "[=================================================>] 100%\n",
      "Epoch 93/100 - 1.77s - loss: 0.0690 - val_loss: 0.0687\n",
      "[=================================================>] 100%\n",
      "Epoch 94/100 - 1.77s - loss: 0.0686 - val_loss: 0.0684\n",
      "[=================================================>] 100%\n",
      "Epoch 95/100 - 1.61s - loss: 0.0682 - val_loss: 0.0680\n",
      "[=================================================>] 100%\n",
      "Epoch 96/100 - 1.47s - loss: 0.0679 - val_loss: 0.0676\n",
      "[=================================================>] 100%\n",
      "Epoch 97/100 - 1.70s - loss: 0.0675 - val_loss: 0.0673\n",
      "[=================================================>] 100%\n",
      "Epoch 98/100 - 1.49s - loss: 0.0671 - val_loss: 0.0669\n",
      "[=================================================>] 100%\n",
      "Epoch 99/100 - 1.60s - loss: 0.0668 - val_loss: 0.0665\n",
      "[=================================================>] 100%\n",
      "Epoch 100/100 - 1.86s - loss: 0.0664 - val_loss: 0.0662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.21853003649069225,\n",
       "  0.1923652552353697,\n",
       "  0.16399459560470275,\n",
       "  0.13871405622584854,\n",
       "  0.12026734534309373,\n",
       "  0.10867156559409694,\n",
       "  0.1018614802219084,\n",
       "  0.09786052886268694,\n",
       "  0.09542530498774192,\n",
       "  0.09387123345655081,\n",
       "  0.09282930508526263,\n",
       "  0.09209616052984919,\n",
       "  0.09155586305122751,\n",
       "  0.09113973269104544,\n",
       "  0.09080561546283465,\n",
       "  0.09052690616168795,\n",
       "  0.09028619196649257,\n",
       "  0.09007181732911608,\n",
       "  0.08987579526716423,\n",
       "  0.08969256602889614,\n",
       "  0.08951819811048638,\n",
       "  0.08934978189025937,\n",
       "  0.08918524113332957,\n",
       "  0.089022958469296,\n",
       "  0.0888617092548051,\n",
       "  0.08870052653033192,\n",
       "  0.08853864644929674,\n",
       "  0.08837545001339851,\n",
       "  0.08821041788283722,\n",
       "  0.08804305105844037,\n",
       "  0.08787304765179331,\n",
       "  0.08770010619560299,\n",
       "  0.08752388176128986,\n",
       "  0.08734407399166749,\n",
       "  0.08716041276471419,\n",
       "  0.08697266674158817,\n",
       "  0.08678069187741157,\n",
       "  0.08658431778156209,\n",
       "  0.0863834200692491,\n",
       "  0.0861778992796666,\n",
       "  0.08596767825456951,\n",
       "  0.08575270687818269,\n",
       "  0.08553287578463506,\n",
       "  0.08530810839631318,\n",
       "  0.0850782473663744,\n",
       "  0.08484326315462251,\n",
       "  0.08460322887580012,\n",
       "  0.08435790536187977,\n",
       "  0.0841072180640635,\n",
       "  0.08385108591854598,\n",
       "  0.08358955704592799,\n",
       "  0.08332249161587031,\n",
       "  0.08304977867978829,\n",
       "  0.08277138245377279,\n",
       "  0.08248735902002025,\n",
       "  0.08219755922946842,\n",
       "  0.08190211535486586,\n",
       "  0.08160109890292776,\n",
       "  0.08129448999145775,\n",
       "  0.08098235223114417,\n",
       "  0.08066484188428809,\n",
       "  0.0803421505247592,\n",
       "  0.08001436461335404,\n",
       "  0.07968151259824253,\n",
       "  0.07934384663343837,\n",
       "  0.07900162273506009,\n",
       "  0.07865504538863424,\n",
       "  0.07830435145180713,\n",
       "  0.077949836192798,\n",
       "  0.07759172433598005,\n",
       "  0.07723023039789578,\n",
       "  0.07686545919892684,\n",
       "  0.07649776152664267,\n",
       "  0.07612749118722802,\n",
       "  0.07575486145093574,\n",
       "  0.07538021220603418,\n",
       "  0.07500374611690493,\n",
       "  0.07462590826983143,\n",
       "  0.07424674483292588,\n",
       "  0.07386654792549281,\n",
       "  0.07348584920352891,\n",
       "  0.0731050003443456,\n",
       "  0.07272421509197437,\n",
       "  0.07234385831055039,\n",
       "  0.0719641123904799,\n",
       "  0.07158526409099224,\n",
       "  0.07120738284158143,\n",
       "  0.07083055140418676,\n",
       "  0.07045490763776208,\n",
       "  0.07008053457952705,\n",
       "  0.06970755586868849,\n",
       "  0.06933602531690672,\n",
       "  0.06896603479007847,\n",
       "  0.06859757609683186,\n",
       "  0.06823061388332041,\n",
       "  0.06786512729491291,\n",
       "  0.06750118235986693,\n",
       "  0.06713867729600109,\n",
       "  0.06677777745553021,\n",
       "  0.06641862921655131],\n",
       " 'val_loss': [0.2062080484552263,\n",
       "  0.17817712092665536,\n",
       "  0.15046596732338993,\n",
       "  0.1282904555249343,\n",
       "  0.11352488198788828,\n",
       "  0.10469939622117694,\n",
       "  0.09955461837960769,\n",
       "  0.09648228860296539,\n",
       "  0.09456518812292647,\n",
       "  0.09330893994618636,\n",
       "  0.09244417230804491,\n",
       "  0.09182042936580849,\n",
       "  0.09134983603301863,\n",
       "  0.0909792921956192,\n",
       "  0.09067567990544505,\n",
       "  0.09041774362051991,\n",
       "  0.09019134874566696,\n",
       "  0.08998687670370506,\n",
       "  0.08979774706756698,\n",
       "  0.08961932763297892,\n",
       "  0.08944821607374265,\n",
       "  0.08928194851232871,\n",
       "  0.08911865805946298,\n",
       "  0.08895697572624482,\n",
       "  0.08879586806401596,\n",
       "  0.0886344501351735,\n",
       "  0.08847201990280876,\n",
       "  0.0883080763511341,\n",
       "  0.08814209600601038,\n",
       "  0.08797359575480591,\n",
       "  0.0878022378710333,\n",
       "  0.08762775541986204,\n",
       "  0.08744978136572949,\n",
       "  0.0872680737491009,\n",
       "  0.08708237811247638,\n",
       "  0.08689259621204704,\n",
       "  0.08669854020956855,\n",
       "  0.08650002158447265,\n",
       "  0.08629692354858275,\n",
       "  0.08608915289998749,\n",
       "  0.08587666532534005,\n",
       "  0.08565939415571189,\n",
       "  0.08543726245669846,\n",
       "  0.08521019301470016,\n",
       "  0.08497793708270321,\n",
       "  0.08474063998627893,\n",
       "  0.08449826571028374,\n",
       "  0.08425047277339594,\n",
       "  0.08399725320517384,\n",
       "  0.08373841519430408,\n",
       "  0.083474026348088,\n",
       "  0.08320407315254906,\n",
       "  0.0829282981969056,\n",
       "  0.08264684802528331,\n",
       "  0.08235970695383503,\n",
       "  0.08206675330903536,\n",
       "  0.0817680862743708,\n",
       "  0.08146377550138298,\n",
       "  0.08115387900873604,\n",
       "  0.08083846423823336,\n",
       "  0.08051779523094789,\n",
       "  0.08019199152487806,\n",
       "  0.07986091441462742,\n",
       "  0.07952475269249513,\n",
       "  0.07918366466439163,\n",
       "  0.07883780664390479,\n",
       "  0.07848760441317464,\n",
       "  0.07813334333979893,\n",
       "  0.07777533907210149,\n",
       "  0.07741382907730074,\n",
       "  0.07704896569601209,\n",
       "  0.07668108733646901,\n",
       "  0.07631059546338953,\n",
       "  0.07593753660066241,\n",
       "  0.07556200475732076,\n",
       "  0.07518436216153607,\n",
       "  0.07480527448290548,\n",
       "  0.07442479657269796,\n",
       "  0.07404308548890887,\n",
       "  0.07366040143122511,\n",
       "  0.07327732500566479,\n",
       "  0.07289419112565335,\n",
       "  0.07251124767264147,\n",
       "  0.07212887147501401,\n",
       "  0.0717473391725689,\n",
       "  0.07136669146231366,\n",
       "  0.07098689922665304,\n",
       "  0.07060816725111968,\n",
       "  0.07023075661529674,\n",
       "  0.06985474184267218,\n",
       "  0.06948023930188732,\n",
       "  0.06910726647178951,\n",
       "  0.06873574064338375,\n",
       "  0.06836563648695503,\n",
       "  0.06799707634057121,\n",
       "  0.06763000411908687,\n",
       "  0.06726452234784437,\n",
       "  0.06690057425712152,\n",
       "  0.06653835011473376,\n",
       "  0.06617788844735392]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork('mse')\n",
    "model.add_layer(DenseLayer(output_size=128, activation=ReLu(), init=\"Xavier\"))\n",
    "model.add_layer(DenseLayer(output_size=64, activation=ReLu(), init=\"Xavier\"))\n",
    "model.add_layer(DenseLayer(output_size=10, activation=Sigmoid(), init=\"Xavier\"))\n",
    "\n",
    "model.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.05,\n",
    "    optimizer=\"gradient_descent\",\n",
    "    isOne_hot=True,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6615\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "pred_classes = np.argmax(predictions, axis=1)\n",
    "accuracy = accuracy_score(pred_classes, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of depth (Number of layers) and Width (Number of neurons per layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed Depth\n",
    "- Number of hidden layers : 2\n",
    "- Test 1 : 64 neurons per layer\n",
    "- Test 2 : 128 neurons per layer\n",
    "- Test 3 : 256 neurons per layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================>] 100%\n",
      "Epoch 1/100 - 1.70s - loss: 0.2490 - val_loss: 0.2324\n",
      "[=================================================>] 100%\n",
      "Epoch 2/100 - 1.71s - loss: 0.2177 - val_loss: 0.2028\n",
      "[=================================================>] 100%\n",
      "Epoch 3/100 - 1.62s - loss: 0.1879 - val_loss: 0.1729\n",
      "[=================================================>] 100%\n",
      "Epoch 4/100 - 1.53s - loss: 0.1591 - val_loss: 0.1461\n",
      "[=================================================>] 100%\n",
      "Epoch 5/100 - 1.98s - loss: 0.1355 - val_loss: 0.1260\n",
      "[=================================================>] 100%\n",
      "Epoch 6/100 - 2.10s - loss: 0.1189 - val_loss: 0.1128\n",
      "[=================================================>] 100%\n",
      "Epoch 7/100 - 1.69s - loss: 0.1085 - val_loss: 0.1048\n",
      "[=================================================>] 100%\n",
      "Epoch 8/100 - 1.91s - loss: 0.1022 - val_loss: 0.1000\n",
      "[=================================================>] 100%\n",
      "Epoch 9/100 - 1.70s - loss: 0.0985 - val_loss: 0.0971\n",
      "[=================================================>] 100%\n",
      "Epoch 10/100 - 1.80s - loss: 0.0961 - val_loss: 0.0953\n",
      "[=================================================>] 100%\n",
      "Epoch 11/100 - 1.57s - loss: 0.0946 - val_loss: 0.0940\n",
      "[=================================================>] 100%\n",
      "Epoch 12/100 - 1.51s - loss: 0.0936 - val_loss: 0.0932\n",
      "[=================================================>] 100%\n",
      "Epoch 13/100 - 1.53s - loss: 0.0929 - val_loss: 0.0926\n",
      "[=================================================>] 100%\n",
      "Epoch 14/100 - 1.47s - loss: 0.0924 - val_loss: 0.0922\n",
      "[=================================================>] 100%\n",
      "Epoch 15/100 - 1.55s - loss: 0.0921 - val_loss: 0.0919\n",
      "[=================================================>] 100%\n",
      "Epoch 16/100 - 1.63s - loss: 0.0918 - val_loss: 0.0916\n",
      "[=================================================>] 100%\n",
      "Epoch 17/100 - 1.61s - loss: 0.0915 - val_loss: 0.0914\n",
      "[=================================================>] 100%\n",
      "Epoch 18/100 - 1.49s - loss: 0.0914 - val_loss: 0.0913\n",
      "[=================================================>] 100%\n",
      "Epoch 19/100 - 1.55s - loss: 0.0912 - val_loss: 0.0911\n",
      "[=================================================>] 100%\n",
      "Epoch 20/100 - 1.56s - loss: 0.0911 - val_loss: 0.0910\n",
      "[=================================================>] 100%\n",
      "Epoch 21/100 - 1.81s - loss: 0.0910 - val_loss: 0.0909\n",
      "[=================================================>] 100%\n",
      "Epoch 22/100 - 1.62s - loss: 0.0909 - val_loss: 0.0908\n",
      "[=================================================>] 100%\n",
      "Epoch 23/100 - 1.88s - loss: 0.0908 - val_loss: 0.0907\n",
      "[=================================================>] 100%\n",
      "Epoch 24/100 - 1.97s - loss: 0.0907 - val_loss: 0.0906\n",
      "[=================================================>] 100%\n",
      "Epoch 25/100 - 1.75s - loss: 0.0906 - val_loss: 0.0906\n",
      "[=================================================>] 100%\n",
      "Epoch 26/100 - 1.60s - loss: 0.0905 - val_loss: 0.0905\n",
      "[=================================================>] 100%\n",
      "Epoch 27/100 - 1.49s - loss: 0.0905 - val_loss: 0.0904\n",
      "[=================================================>] 100%\n",
      "Epoch 28/100 - 1.56s - loss: 0.0904 - val_loss: 0.0903\n",
      "[=================================================>] 100%\n",
      "Epoch 29/100 - 1.43s - loss: 0.0903 - val_loss: 0.0903\n",
      "[=================================================>] 100%\n",
      "Epoch 30/100 - 1.76s - loss: 0.0902 - val_loss: 0.0902\n",
      "[=================================================>] 100%\n",
      "Epoch 31/100 - 1.65s - loss: 0.0902 - val_loss: 0.0901\n",
      "[=================================================>] 100%\n",
      "Epoch 32/100 - 1.76s - loss: 0.0901 - val_loss: 0.0901\n",
      "[=================================================>] 100%\n",
      "Epoch 33/100 - 1.64s - loss: 0.0900 - val_loss: 0.0900\n",
      "[=================================================>] 100%\n",
      "Epoch 34/100 - 1.77s - loss: 0.0900 - val_loss: 0.0899\n",
      "[=================================================>] 100%\n",
      "Epoch 35/100 - 1.53s - loss: 0.0899 - val_loss: 0.0898\n",
      "[=================================================>] 100%\n",
      "Epoch 36/100 - 1.50s - loss: 0.0898 - val_loss: 0.0898\n",
      "[=================================================>] 100%\n",
      "Epoch 37/100 - 1.53s - loss: 0.0897 - val_loss: 0.0897\n",
      "[=================================================>] 100%\n",
      "Epoch 38/100 - 1.60s - loss: 0.0897 - val_loss: 0.0896\n",
      "[=================================================>] 100%\n",
      "Epoch 39/100 - 1.55s - loss: 0.0896 - val_loss: 0.0896\n",
      "[=================================================>] 100%\n",
      "Epoch 40/100 - 1.43s - loss: 0.0895 - val_loss: 0.0895\n",
      "[=================================================>] 100%\n",
      "Epoch 41/100 - 1.48s - loss: 0.0895 - val_loss: 0.0894\n",
      "[=================================================>] 100%\n",
      "Epoch 42/100 - 1.68s - loss: 0.0894 - val_loss: 0.0893\n",
      "[=================================================>] 100%\n",
      "Epoch 43/100 - 1.52s - loss: 0.0893 - val_loss: 0.0893\n",
      "[=================================================>] 100%\n",
      "Epoch 44/100 - 1.91s - loss: 0.0892 - val_loss: 0.0892\n",
      "[=================================================>] 100%\n",
      "Epoch 45/100 - 1.61s - loss: 0.0891 - val_loss: 0.0891\n",
      "[=================================================>] 100%\n",
      "Epoch 46/100 - 1.47s - loss: 0.0891 - val_loss: 0.0890\n",
      "[=================================================>] 100%\n",
      "Epoch 47/100 - 1.47s - loss: 0.0890 - val_loss: 0.0889\n",
      "[=================================================>] 100%\n",
      "Epoch 48/100 - 1.47s - loss: 0.0889 - val_loss: 0.0888\n",
      "[=================================================>] 100%\n",
      "Epoch 49/100 - 1.47s - loss: 0.0888 - val_loss: 0.0888\n",
      "[=================================================>] 100%\n",
      "Epoch 50/100 - 1.53s - loss: 0.0887 - val_loss: 0.0887\n",
      "[=================================================>] 100%\n",
      "Epoch 51/100 - 1.42s - loss: 0.0886 - val_loss: 0.0886\n",
      "[=================================================>] 100%\n",
      "Epoch 52/100 - 1.52s - loss: 0.0885 - val_loss: 0.0885\n",
      "[=================================================>] 100%\n",
      "Epoch 53/100 - 1.55s - loss: 0.0884 - val_loss: 0.0884\n",
      "[=================================================>] 100%\n",
      "Epoch 54/100 - 1.56s - loss: 0.0883 - val_loss: 0.0883\n",
      "[=================================================>] 100%\n",
      "Epoch 55/100 - 1.57s - loss: 0.0883 - val_loss: 0.0882\n",
      "[=================================================>] 100%\n",
      "Epoch 56/100 - 1.58s - loss: 0.0882 - val_loss: 0.0881\n",
      "[=================================================>] 100%\n",
      "Epoch 57/100 - 1.45s - loss: 0.0880 - val_loss: 0.0880\n",
      "[=================================================>] 100%\n",
      "Epoch 58/100 - 1.50s - loss: 0.0879 - val_loss: 0.0879\n",
      "[=================================================>] 100%\n",
      "Epoch 59/100 - 1.53s - loss: 0.0878 - val_loss: 0.0878\n",
      "[=================================================>] 100%\n",
      "Epoch 60/100 - 1.49s - loss: 0.0877 - val_loss: 0.0877\n",
      "[=================================================>] 100%\n",
      "Epoch 61/100 - 1.44s - loss: 0.0876 - val_loss: 0.0875\n",
      "[=================================================>] 100%\n",
      "Epoch 62/100 - 1.46s - loss: 0.0875 - val_loss: 0.0874\n",
      "[=================================================>] 100%\n",
      "Epoch 63/100 - 1.58s - loss: 0.0874 - val_loss: 0.0873\n",
      "[=================================================>] 100%\n",
      "Epoch 64/100 - 1.50s - loss: 0.0873 - val_loss: 0.0872\n",
      "[=================================================>] 100%\n",
      "Epoch 65/100 - 1.56s - loss: 0.0871 - val_loss: 0.0871\n",
      "[=================================================>] 100%\n",
      "Epoch 66/100 - 1.58s - loss: 0.0870 - val_loss: 0.0869\n",
      "[=================================================>] 100%\n",
      "Epoch 67/100 - 1.40s - loss: 0.0869 - val_loss: 0.0868\n",
      "[=================================================>] 100%\n",
      "Epoch 68/100 - 1.61s - loss: 0.0867 - val_loss: 0.0867\n",
      "[=================================================>] 100%\n",
      "Epoch 69/100 - 1.63s - loss: 0.0866 - val_loss: 0.0865\n",
      "[=================================================>] 100%\n",
      "Epoch 70/100 - 1.41s - loss: 0.0865 - val_loss: 0.0864\n",
      "[=================================================>] 100%\n",
      "Epoch 71/100 - 1.56s - loss: 0.0863 - val_loss: 0.0862\n",
      "[=================================================>] 100%\n",
      "Epoch 72/100 - 1.65s - loss: 0.0862 - val_loss: 0.0861\n",
      "[=================================================>] 100%\n",
      "Epoch 73/100 - 1.46s - loss: 0.0860 - val_loss: 0.0859\n",
      "[=================================================>] 100%\n",
      "Epoch 74/100 - 1.40s - loss: 0.0859 - val_loss: 0.0858\n",
      "[=================================================>] 100%\n",
      "Epoch 75/100 - 1.53s - loss: 0.0857 - val_loss: 0.0856\n",
      "[=================================================>] 100%\n",
      "Epoch 76/100 - 1.47s - loss: 0.0856 - val_loss: 0.0855\n",
      "[=================================================>] 100%\n",
      "Epoch 77/100 - 1.39s - loss: 0.0854 - val_loss: 0.0853\n",
      "[=================================================>] 100%\n",
      "Epoch 78/100 - 1.51s - loss: 0.0852 - val_loss: 0.0851\n",
      "[=================================================>] 100%\n",
      "Epoch 79/100 - 1.53s - loss: 0.0851 - val_loss: 0.0849\n",
      "[=================================================>] 100%\n",
      "Epoch 80/100 - 1.38s - loss: 0.0849 - val_loss: 0.0848\n",
      "[=================================================>] 100%\n",
      "Epoch 81/100 - 1.59s - loss: 0.0847 - val_loss: 0.0846\n",
      "[=================================================>] 100%\n",
      "Epoch 82/100 - 1.72s - loss: 0.0845 - val_loss: 0.0844\n",
      "[=================================================>] 100%\n",
      "Epoch 83/100 - 1.70s - loss: 0.0843 - val_loss: 0.0842\n",
      "[=================================================>] 100%\n",
      "Epoch 84/100 - 1.49s - loss: 0.0841 - val_loss: 0.0840\n",
      "[=================================================>] 100%\n",
      "Epoch 85/100 - 1.62s - loss: 0.0839 - val_loss: 0.0838\n",
      "[=================================================>] 100%\n",
      "Epoch 86/100 - 1.55s - loss: 0.0837 - val_loss: 0.0836\n",
      "[=================================================>] 100%\n",
      "Epoch 87/100 - 1.58s - loss: 0.0835 - val_loss: 0.0834\n",
      "[=================================================>] 100%\n",
      "Epoch 88/100 - 1.52s - loss: 0.0833 - val_loss: 0.0832\n",
      "[=================================================>] 100%\n",
      "Epoch 89/100 - 1.51s - loss: 0.0831 - val_loss: 0.0830\n",
      "[=================================================>] 100%\n",
      "Epoch 90/100 - 1.50s - loss: 0.0829 - val_loss: 0.0827\n",
      "[=================================================>] 100%\n",
      "Epoch 91/100 - 1.59s - loss: 0.0826 - val_loss: 0.0825\n",
      "[=================================================>] 100%\n",
      "Epoch 92/100 - 1.53s - loss: 0.0824 - val_loss: 0.0823\n",
      "[=================================================>] 100%\n",
      "Epoch 93/100 - 1.56s - loss: 0.0822 - val_loss: 0.0820\n",
      "[=================================================>] 100%\n",
      "Epoch 94/100 - 1.44s - loss: 0.0819 - val_loss: 0.0818\n",
      "[=================================================>] 100%\n",
      "Epoch 95/100 - 1.55s - loss: 0.0817 - val_loss: 0.0816\n",
      "[=================================================>] 100%\n",
      "Epoch 96/100 - 1.51s - loss: 0.0815 - val_loss: 0.0813\n",
      "[=================================================>] 100%\n",
      "Epoch 97/100 - 1.48s - loss: 0.0812 - val_loss: 0.0810\n",
      "[=================================================>] 100%\n",
      "Epoch 98/100 - 1.59s - loss: 0.0809 - val_loss: 0.0808\n",
      "[=================================================>] 100%\n",
      "Epoch 99/100 - 1.56s - loss: 0.0807 - val_loss: 0.0805\n",
      "[=================================================>] 100%\n",
      "Epoch 100/100 - 1.53s - loss: 0.0804 - val_loss: 0.0803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.24896870999864829,\n",
       "  0.21772392982372143,\n",
       "  0.1878529883690506,\n",
       "  0.15913406169171143,\n",
       "  0.13547849554725733,\n",
       "  0.11888395854282767,\n",
       "  0.10846971771868423,\n",
       "  0.10221966329036776,\n",
       "  0.09845411889933237,\n",
       "  0.09612035797413274,\n",
       "  0.09461976627607036,\n",
       "  0.09361719290638693,\n",
       "  0.09292223669802564,\n",
       "  0.09242344049592666,\n",
       "  0.09205330529661417,\n",
       "  0.0917698732809284,\n",
       "  0.09154618515040751,\n",
       "  0.09136444076693685,\n",
       "  0.09121272032420445,\n",
       "  0.09108276012158575,\n",
       "  0.09096874825612844,\n",
       "  0.090866563289696,\n",
       "  0.0907732190541417,\n",
       "  0.09068650264306288,\n",
       "  0.09060473894455172,\n",
       "  0.09052672685408471,\n",
       "  0.09045152268771581,\n",
       "  0.09037839559564889,\n",
       "  0.09030676406045215,\n",
       "  0.09023614192226592,\n",
       "  0.09016616182642946,\n",
       "  0.09009653175515227,\n",
       "  0.09002701615739579,\n",
       "  0.08995740807194884,\n",
       "  0.08988751027646762,\n",
       "  0.08981716356881375,\n",
       "  0.0897462286851297,\n",
       "  0.0896745669906275,\n",
       "  0.08960207383714722,\n",
       "  0.08952864436498795,\n",
       "  0.08945421173968954,\n",
       "  0.08937866051272636,\n",
       "  0.08930186656640292,\n",
       "  0.08922373111705265,\n",
       "  0.0891441705885225,\n",
       "  0.08906306442489904,\n",
       "  0.08898033932809185,\n",
       "  0.08889591168356167,\n",
       "  0.08880971153115627,\n",
       "  0.08872166403281294,\n",
       "  0.08863172372453822,\n",
       "  0.08853976001873276,\n",
       "  0.08844573376091991,\n",
       "  0.0883495966796624,\n",
       "  0.08825125345284952,\n",
       "  0.08815063131780276,\n",
       "  0.08804769798636379,\n",
       "  0.08794236679871303,\n",
       "  0.08783462128571945,\n",
       "  0.08772439794829012,\n",
       "  0.08761157771240556,\n",
       "  0.08749616145266935,\n",
       "  0.08737809471295384,\n",
       "  0.08725727136934447,\n",
       "  0.08713361816669374,\n",
       "  0.08700703215789486,\n",
       "  0.08687740781690453,\n",
       "  0.08674468418743997,\n",
       "  0.08660873344590234,\n",
       "  0.08646946711581584,\n",
       "  0.08632681610398195,\n",
       "  0.08618066936170582,\n",
       "  0.08603087989605133,\n",
       "  0.08587742815681264,\n",
       "  0.08572031326571923,\n",
       "  0.0855594509832944,\n",
       "  0.0853948275961087,\n",
       "  0.08522629167899076,\n",
       "  0.08505372292156418,\n",
       "  0.08487711932635454,\n",
       "  0.08469632266231011,\n",
       "  0.08451129774788134,\n",
       "  0.08432200977965114,\n",
       "  0.08412837343624598,\n",
       "  0.08393033149141123,\n",
       "  0.08372779808534345,\n",
       "  0.08352076776548331,\n",
       "  0.08330924554973057,\n",
       "  0.08309321961439703,\n",
       "  0.08287268972047929,\n",
       "  0.08264767140664708,\n",
       "  0.08241806537429544,\n",
       "  0.08218390789459888,\n",
       "  0.08194526188353168,\n",
       "  0.08170214355167871,\n",
       "  0.08145452671596703,\n",
       "  0.08120248705797177,\n",
       "  0.08094610324058925,\n",
       "  0.08068539983547594,\n",
       "  0.08042047768106723],\n",
       " 'val_loss': [0.23243722846093587,\n",
       "  0.20282613009590397,\n",
       "  0.17290189366180886,\n",
       "  0.14614596709184408,\n",
       "  0.1259745253593214,\n",
       "  0.11279057342300836,\n",
       "  0.10479486954305504,\n",
       "  0.10001431986561567,\n",
       "  0.09709691310253098,\n",
       "  0.09525377987765801,\n",
       "  0.09404414028061918,\n",
       "  0.09321963715012976,\n",
       "  0.09263729613768248,\n",
       "  0.09221193568096618,\n",
       "  0.09189104012084642,\n",
       "  0.09164138716002118,\n",
       "  0.09144132060565485,\n",
       "  0.09127643455045639,\n",
       "  0.09113687657100632,\n",
       "  0.09101577920154685,\n",
       "  0.09090830386761245,\n",
       "  0.09081097594574426,\n",
       "  0.09072122261231787,\n",
       "  0.0906371620887999,\n",
       "  0.09055737403063396,\n",
       "  0.0904808251354736,\n",
       "  0.09040667668906209,\n",
       "  0.09033429199208397,\n",
       "  0.09026314421084213,\n",
       "  0.0901928068515804,\n",
       "  0.09012294897514252,\n",
       "  0.09005329174266581,\n",
       "  0.08998364060208275,\n",
       "  0.08991379488254872,\n",
       "  0.08984355726898666,\n",
       "  0.08977281092959988,\n",
       "  0.08970140103358201,\n",
       "  0.0896292367348274,\n",
       "  0.08955620276325293,\n",
       "  0.08948218278019586,\n",
       "  0.0894071030578969,\n",
       "  0.08933082047105123,\n",
       "  0.08925323611763107,\n",
       "  0.08917428330259891,\n",
       "  0.08909383329948571,\n",
       "  0.0890118228119953,\n",
       "  0.08892815333842434,\n",
       "  0.08884278599010831,\n",
       "  0.0887555969449068,\n",
       "  0.08866651365588024,\n",
       "  0.0885753940317695,\n",
       "  0.08848218837730142,\n",
       "  0.08838689579053065,\n",
       "  0.08828948155235708,\n",
       "  0.08818985895317225,\n",
       "  0.08808797367936685,\n",
       "  0.08798376190742452,\n",
       "  0.08787710691399508,\n",
       "  0.08776796085845959,\n",
       "  0.08765623835410385,\n",
       "  0.08754187341445833,\n",
       "  0.08742481967278365,\n",
       "  0.08730507942983577,\n",
       "  0.08718260950186385,\n",
       "  0.08705719083876427,\n",
       "  0.08692894021079234,\n",
       "  0.0867976812905464,\n",
       "  0.08666319941356398,\n",
       "  0.08652542315256133,\n",
       "  0.08638431308950603,\n",
       "  0.08623976645433126,\n",
       "  0.08609168203410547,\n",
       "  0.08593992162999048,\n",
       "  0.0857844654388672,\n",
       "  0.08562531833740017,\n",
       "  0.08546236659238023,\n",
       "  0.08529559381738774,\n",
       "  0.08512478972941986,\n",
       "  0.08494992272428949,\n",
       "  0.08477092460656567,\n",
       "  0.08458773359171308,\n",
       "  0.08440027751674994,\n",
       "  0.08420849091165246,\n",
       "  0.08401232037389546,\n",
       "  0.08381179774896655,\n",
       "  0.08360666019404224,\n",
       "  0.08339697678150323,\n",
       "  0.08318269282257157,\n",
       "  0.08296381297942447,\n",
       "  0.08274034429918574,\n",
       "  0.08251227022378625,\n",
       "  0.08227954338034035,\n",
       "  0.08204227621506235,\n",
       "  0.08180055005424133,\n",
       "  0.08155435254313817,\n",
       "  0.08130368634010827,\n",
       "  0.08104860767811414,\n",
       "  0.08078922738385078,\n",
       "  0.080525604757252,\n",
       "  0.08025788058797835]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test_1_fixed_depth = NeuralNetwork('mse')\n",
    "model_test_1_fixed_depth.add_layer(DenseLayer(output_size=128, activation=ReLu(), init=\"Xavier\"))\n",
    "model_test_1_fixed_depth.add_layer(DenseLayer(output_size=64, activation=ReLu(), init=\"Xavier\"))\n",
    "model_test_1_fixed_depth.add_layer(DenseLayer(output_size=10, activation=Sigmoid(), init=\"Xavier\"))\n",
    "\n",
    "model_test_1_fixed_depth.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.05,\n",
    "    optimizer=\"gradient_descent\",\n",
    "    isOne_hot=True,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5054\n"
     ]
    }
   ],
   "source": [
    "predictions_test_1_fixed_depth = model_test_1_fixed_depth.predict(X_test)\n",
    "pred_classes_test_1_fixed_depth = np.argmax(predictions_test_1_fixed_depth, axis=1)\n",
    "accuracy_test_1_fixed_depth = accuracy_score(pred_classes_test_1_fixed_depth, y_test)\n",
    "print(\"Test Accuracy:\", accuracy_test_1_fixed_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare model with sklearn MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37724840\n",
      "Iteration 2, loss = 0.15004672\n",
      "Iteration 3, loss = 0.10562821\n",
      "Iteration 4, loss = 0.08064147\n",
      "Iteration 5, loss = 0.06428497\n",
      "Iteration 6, loss = 0.05121985\n",
      "Iteration 7, loss = 0.04219482\n",
      "Iteration 8, loss = 0.03723277\n",
      "Iteration 9, loss = 0.02986735\n",
      "Iteration 10, loss = 0.02325205\n",
      "Iteration 11, loss = 0.02124234\n",
      "Iteration 12, loss = 0.01549812\n",
      "Iteration 13, loss = 0.01433403\n",
      "Iteration 14, loss = 0.01377085\n",
      "Iteration 15, loss = 0.00927998\n",
      "Iteration 16, loss = 0.00973711\n",
      "Iteration 17, loss = 0.01097088\n",
      "Iteration 18, loss = 0.00895768\n",
      "Iteration 19, loss = 0.00620646\n",
      "Iteration 20, loss = 0.00376964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=20, random_state=1,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=20, random_state=1,\n",
       "              verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=20, random_state=1,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(128, 64),activation='relu', \n",
    "                    solver='adam', max_iter=20, random_state=1, verbose=True)\n",
    "\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier Test Accuracy: 0.9790\n"
     ]
    }
   ],
   "source": [
    "y_pred_mlp = mlp.predict(X_test)\n",
    "acc_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "print(f\"MLPClassifier Test Accuracy: {acc_mlp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       986\n",
      "           1       0.99      0.99      0.99      1125\n",
      "           2       0.97      0.98      0.98       999\n",
      "           3       0.99      0.95      0.97      1020\n",
      "           4       0.98      0.98      0.98       975\n",
      "           5       0.96      0.98      0.97       902\n",
      "           6       0.99      0.99      0.99       982\n",
      "           7       0.98      0.98      0.98      1042\n",
      "           8       0.98      0.97      0.97       975\n",
      "           9       0.95      0.99      0.97       994\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
